{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.使用神经网络对手写数字做分类"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Mnist数据集\n",
    "MNIST数据集改编自美国国家标准与技术研究所收集的NIST数据集。该数据集收集了来自250个不同人手写的数字图片，其中一半是人口普查局的工作人员，另一半是高中生。该数据集包括50000张训练集图片和10000张测试集图片，训练集和测试集都提供了正确答案。每张图片都是28×28=784大小的灰度图片，也就是一个28×28的矩阵，里面每个值是一个像素点，值在[0,1]之间，其中0表示白色，1表示黑色。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.读取Mnist数据集\n",
    "pkl是python内置的一种格式，可以将python的各种数据结构序列化存储到磁盘中，需要时又可以读取并反序列化到内存中。mnist.pkl.gz做了两次操作，先pkl序列化，再gz压缩存储，所以要读取该文件，需要先解压再反序列化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path(\"../data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train训练样本，是一个50000×784的矩阵，表示有50000个训练样本，每个训练样本是一个784的一维数组，784就是把一张28×28的图片展开成的一维数组\n",
    "print(f\"x_train:{x_train}\")\n",
    "print(f\"x_train.shape:{x_train.shape}\") # shape获取 矩阵的维度属性\n",
    "# x_train训练样本对应的识别答案\n",
    "print(f\"y_train:{y_train}\")\n",
    "print(f\"y_train.shape:{y_train.shape}\")\n",
    "print(\"\\n\")\n",
    "print(f\"x_valid:{x_valid}\")\n",
    "print(f\"x_valid.shape:{x_valid.shape}\")\n",
    "print(f\"y_valid:{y_train}\") \n",
    "print(f\"y_valid.shape:{y_valid.shape}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.使用matplotlib还原28*28矩阵对应的灰度图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the data\n",
    "fig, ax = plt.subplots(1, 6, figsize=(24, 4))\n",
    "\n",
    "for i in range(6):\n",
    "    ax[i].imshow(x_train[i].reshape((28, 28)), cmap='gray')\n",
    "    ax[i].set_title(f'x_train[{i}]')\n",
    "\n",
    "print(f'y_train[:6]:{y_train[:6]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.将数据需转换成tensor以便后续建模训练\n",
    "Tensor是PyTorch中的基本数据类型，它是一个多维数组，可以用来表示向量、矩阵、甚至是更高维的数据。Tensor类似于Numpy中的数组，但是它可以在GPU上进行计算，从而加速计算过程。\n",
    "\n",
    "在PyTorch中，我们可以使用Tensor来构建神经网络。通过多层Tensor的组合和非线性变换，我们可以实现复杂的非线性映射关系，从而对数据进行分类、识别、聚类等操作。\n",
    "\n",
    "与Numpy中的数组类似，Tensor可以使用各种各样的操作进行计算，例如加法、乘法、矩阵乘法等。此外，PyTorch还提供了许多高级的操作，例如卷积、池化等等，可以方便地构建深度学习模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")\n",
    "\n",
    "print(f\"x_train转换tensor后的结构: {x_train}\")\n",
    "print(x_train.shape)\n",
    "print(f\"y_train转换tensor后的结构: {y_train}\")\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.创建模型\n",
    "1. FC = Wx+B：FC代表全连接层，表示输入向量x通过权重矩阵W（weight）进行线性变换，并加上偏差向量B(bias)，最终得到全连接层的输出。\n",
    "2. nn.Module：用于定义神经网络的层、激活函数、损失函数等组件，可以方便地构建自定义神经网络。每个nn.Module实例都有一个`forward`方法，用于定义数据在神经网络中的正向传播过程。通过继承nn.Module类并实现forward方法，我们可以构建自己的神经网络。\n",
    "3. nn.Linear：用于定义全连接层。它有两个参数`in_features`和`out_features`，分别表示输入特征的维度和输出特征的维度。通过实例化Linear，我们可以方便地构建全连接层，并将其作为神经网络的一部分来训练。\n",
    "4. dropout：在训练过程中随机丢弃一部分神经元的输出来减少过拟合，一般每个全连接层都要加一个dropout。\n",
    "5. F.relu：激活函数，负责将输入的神经元转换为输出。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Mnist_NN(nn.Module): # 继承nn.Module\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden1 = nn.Linear(784, 128) # 全连接层1  输入784个像素点 输出128个特征\n",
    "        self.hidden2 = nn.Linear(128, 256) # 全连接层2 输入128 输出256\n",
    "        self.dropout = nn.Dropout(0.5) # 随机杀死一半，防止过拟合\n",
    "        self.out = nn.Linear(256, 10) # 输出层  输入256 输出10个类别分别的概率\n",
    "\n",
    "    # 前向传播\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "# 打印参数  \n",
    "for name, parameter in Mnist_NN().named_parameters():\n",
    "    print(f\"参数名字:{name}, 参数矩阵的大小{parameter.size()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.打包数据：TensorDataset和DataLoader\n",
    "1. TensorDataset：是一个用于包装张量的类，可以将多个张量打包成一个数据集。例如，我们可以将训练数据和对应的标签打包成一个TensorDataset对象，便于后续的数据处理和模型训练。\n",
    "2. DataLoader：是一个用于加载数据的类，可以从TensorDataset对象中加载数据，并将其转换为可用于训练的批量数据。DataLoader类的实例可以指定批量大小、数据的采样方式、数据的并行加载方式等参数，可以方便地处理大规模数据集，并加速模型训练的过程。\n",
    "\n",
    "在PyTorch中，通常使用TensorDataset和DataLoader配合使用，以便在训练过程中高效地加载和处理数据。具体来说，我们可以先将数据和标签打包成一个TensorDataset对象，然后使用DataLoader类来加载数据，并在训练过程中逐批地处理数据。这样可以方便地处理大规模数据集，并加速模型训练的过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "bs = 64 # 每次迭代训练的样本数\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)\n",
    "\n",
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )\n",
    "\n",
    "\n",
    "for xb, yb in train_dl:\n",
    "    print(xb.shape) # 把数据打包成64大小的包\n",
    "    print(yb.shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 训练"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. zip用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]\n",
    "b = [4, 5, 6]\n",
    "\n",
    "zipped = zip(a, b)\n",
    "print(zipped)\n",
    "print(list(zipped)) \n",
    "\n",
    "a2, b2 = zip(*zip(a, b))\n",
    "print(a2)\n",
    "print(b2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.训练模型\n",
    "\n",
    "opt优化器:\n",
    "- optim.Adam：是一种常用的随机梯度下降算法，具有自适应学习率和动量的特点，可以在处理大规模数据集和高维参数空间时保持良好的性能。\n",
    "- optim.SGD：用于实现随机梯度下降优化算法。在SGD算法中，每次迭代时，我们随机选择一个样本或一批样本，并使用其梯度来更新模型的参数。通过使用随机梯度下降算法，我们可以逐渐优化模型的参数，并使其逼近最优解。\n",
    "F.cross_entropy：交叉熵损失函数通常用于衡量模型的预测结果与真实标签之间的差距，一般用于多分类问题。我们需要将模型的输出结果与真实标签传递给该函数，然后该函数会自动计算交叉熵损失函数，并返回计算结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import optim\n",
    "\n",
    "def get_model():\n",
    "    model = Mnist_NN()\n",
    "    # 返回模型和优化器，\n",
    "    return model, optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    # 计算损失\n",
    "    loss = loss_func(model(xb), yb) # model(xb) 预测值 yb 真实值\n",
    "\n",
    "    if opt is not None: # 更新模型参数\n",
    "        loss.backward() # 更新梯度\n",
    "        opt.step() # 执行更新\n",
    "        opt.zero_grad() # 单次更新完梯度置0 每次迭代独立去操作\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "# 训练\n",
    "def fit(steps, model, loss_func, opt, train_dl, valid_dl):\n",
    "    \"\"\"\n",
    "        steps 迭代次数  \n",
    "        model 模型 \n",
    "        loss_func 损失函数 \n",
    "        opt 优化器\n",
    "        train_dl 训练打包器\n",
    "        valid_dl 验证打包器\n",
    "    \"\"\"\n",
    "    for step in range(steps):\n",
    "        model.train() # 训练, 更新每一层的w、b\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval() # 验证\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        # 平均损失\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        print('当前step:'+str(step), '验证集损失：'+str(val_loss))\n",
    "\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs) # 数据\n",
    "model, opt = get_model() # 模型\n",
    "loss_func = F.cross_entropy # 损失函数 = 交叉熵\n",
    "fit(20, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 验证\n",
    "torch.max：用于计算张量沿指定维度的最大值，它接受两个参数，第一个参数是要计算最大值的张量，第二个参数是指定的维度。返回两个张量，第一个张量是最大值的值，第二个张量是最大值的索引。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 测试验证集部分数据的准确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制出验证集前12个28*28矩阵对应的灰度图片\n",
    "fig, ax = plt.subplots(1, 12, figsize=(48, 4))\n",
    "for i in range(12):\n",
    "    ax[i].imshow(x_valid[i].reshape((28, 28)), cmap='gray')\n",
    "    ax[i].set_title(f'x_valid[{i}]')\n",
    "\n",
    "\n",
    "# 打印部分模型预测数据\n",
    "for xb, _ in valid_dl:\n",
    "    outputs = model(xb)\n",
    "    _, predicted = torch.max(outputs.data, 1) # 最大值和索引\n",
    "    print(f\"预测值：{predicted[:12]}\" )\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.计算准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for xb, yb in valid_dl:\n",
    "    outputs = model(xb)\n",
    "    _, predicted = torch.max(outputs.data, 1) # 最大值和索引\n",
    "    total += yb.size(0)\n",
    "    correct += (predicted == yb).sum().item()\n",
    "print(\"测试集准确率：%d %%\" % (100*correct/total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
